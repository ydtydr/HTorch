{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "import math\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from HTorch.MCTensor.MCOpBasics import _Renormalize, _Simple_renormalize_old, _Grow_ExpN, _AddMCN,  _ScalingN,\\\n",
    "    _DivMCN, _MultMCN, _exp, _pow, _square, _sinh, _cosh, _tanh, _log, _exp, _sqrt, \\\n",
    "    _softmax, _log_softmax, _cross_entropy, _mse_loss, _layer_norm, _atanh, _log1p_standard, \\\n",
    "    _clamp, _norm, _sum, _mean\n",
    "from HTorch.MCTensor.MCOpMatrix import _Dot_MCN, _MV_MC_T, _MV_T_MC, _MM_MC_T, _MM_T_MC,\\\n",
    "    _BMM_MC_T, _BMM_T_MC, _4DMM_T_MC, _4DMM_MC_T\n",
    "from typing import Union, List\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HANDLED_FUNCTIONS = {}\n",
    "\n",
    "def implements(torch_function):\n",
    "    \"\"\"Register a torch function override for MCTensor\"\"\"\n",
    "    @functools.wraps(torch_function)\n",
    "    def decorator(func):\n",
    "        HANDLED_FUNCTIONS[torch_function] = func\n",
    "        return func\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTensor(Tensor):\n",
    "    @staticmethod \n",
    "    def __new__(cls, *args,  nc=1, **kwargs): \n",
    "        ret = super().__new__(cls, *args, **kwargs)\n",
    "        ret._nc = nc\n",
    "        ret.res = torch.zeros(ret.size() + (nc-1,), dtype=ret.dtype, device=ret.device)\n",
    "        return ret\n",
    "\n",
    "    def __init__(self, *args,  nc=1, **kwargs):\n",
    "        self._nc = nc\n",
    "        self.res = torch.zeros(self.size() + (nc-1,), dtype=self.dtype, device=self.device)\n",
    "        \n",
    "    @staticmethod\n",
    "    def wrap_tensor_to_mctensor(tensor: Tensor) -> MCTensor:\n",
    "        # involves copying\n",
    "        ret = MCTensor(tensor[..., 0], nc=tensor.size(-1))\n",
    "        ret.res.data.copy_(tensor[..., 1:].data)\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_tensor_and_res_to_mctensor(val: Tensor, res: Tensor) -> MCTensor:\n",
    "        # involves copying\n",
    "        ret = MCTensor(val, nc=res.shape[-1] + 1)\n",
    "        ret.res.data.copy_(res.data)\n",
    "        return ret\n",
    "\n",
    "    @property\n",
    "    def tensor(self):\n",
    "        return torch.cat([self.as_subclass(Tensor).view(*self.shape, 1), self.res], -1)\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        return torch.transpose(self, 0, 1)\n",
    "    \n",
    "    @property\n",
    "    def nc(self):\n",
    "        return self._nc\n",
    "        \n",
    "    def normalize_(self, simple=False):\n",
    "        if simple:\n",
    "            normalized_self = _Simple_renormalize_old(self.tensor, self.nc)\n",
    "        else:\n",
    "            normalized_self = _Renormalize(self.tensor, self.nc)\n",
    "        self.data.copy_(normalized_self[..., 0].data)\n",
    "        self.res.data.copy_(normalized_self[..., 1:].data)\n",
    "    \n",
    "    def __repr__(self, *args, **kwargs):\n",
    "        return \"{}, nc={}\".format(super().__repr__(), self.nc)\n",
    "\n",
    "    def __add__(self, other) -> MCTensor:\n",
    "        ''' add self with other'''\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__add__(other)\n",
    "        else:\n",
    "            obj = torch.add(self, other)\n",
    "        return obj\n",
    "\n",
    "    def __radd__(self, other) -> MCTensor:\n",
    "        ''' add self with other'''\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__radd__(other)\n",
    "        else:\n",
    "            obj = torch.add(self, other)\n",
    "        return obj\n",
    "    \n",
    "    def __sub__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__sub__(other)\n",
    "        else:\n",
    "            obj = torch.add(self, -other)\n",
    "        return obj\n",
    "\n",
    "    def __rsub__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__rsub__(other)\n",
    "        else:\n",
    "            obj = torch.add(other, -self)\n",
    "        return obj\n",
    "\n",
    "    def __mul__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__mul__(other)\n",
    "        else:\n",
    "            obj = torch.mul(self, other)\n",
    "        return obj\n",
    "\n",
    "    def __rmul__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__rmul__(other)\n",
    "        else:\n",
    "            obj = torch.mul(other, self)\n",
    "        return obj\n",
    "    \n",
    "    def __truediv__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__truediv__(other)\n",
    "        else:\n",
    "            obj = torch.div(self, other)\n",
    "        return obj\n",
    "\n",
    "    def __rtruediv__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__rtruediv__(other)\n",
    "        else:\n",
    "            obj = torch.div(other, self)\n",
    "        return obj\n",
    "\n",
    "    def __matmul__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__matmul__(other)\n",
    "        else:\n",
    "            obj = torch.matmul(self, other)\n",
    "        return obj\n",
    "\n",
    "    def __rmatmul__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__rmatmul__(other)\n",
    "        else:\n",
    "            obj = torch.matmul(other, self)\n",
    "        return obj\n",
    "\n",
    "    def __pow__(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            obj = super().__pow__(other)\n",
    "        else:\n",
    "            obj = torch.pow(self, other)\n",
    "        return obj\n",
    "\n",
    "    def __setitem__(self, key, value: Union[MCTensor, Tensor, int, float]):\n",
    "        if isinstance(key, tuple) and Ellipsis in key:\n",
    "            res_key = (*key, slice(None, None, None))\n",
    "        else:\n",
    "            res_key = key\n",
    "        if isinstance(value, MCTensor):\n",
    "            super().__setitem__(key, value.as_subclass(Tensor))\n",
    "            self.res[res_key].data.copy_(value.res.data)\n",
    "        else:\n",
    "            super().__setitem__(key, value)\n",
    "            self.res[res_key] = 0\n",
    "\n",
    "    def dot(self, other) -> MCTensor:\n",
    "        if self.nc == 1 and (not isinstance(other, MCTensor) or other.nc == 1):\n",
    "            if isinstance(other, MCTensor):\n",
    "                other = other.as_subclass(Tensor)\n",
    "            obj = super().as_subclass(Tensor).dot(other)\n",
    "        else:\n",
    "            obj = torch.dot(self, other)\n",
    "        return obj\n",
    "    \n",
    "    def abs(self) -> MCTensor:\n",
    "        return torch.abs(self)\n",
    "    \n",
    "    def sum(self, dim=None, keepdim=False, **kw) -> MCTensor:\n",
    "        return torch.sum(self, dim=dim, keepdim=keepdim)\n",
    "    \n",
    "    def mean(self, dim=None, keepdim=False, **kw) -> MCTensor:\n",
    "        return torch.mean(self, dim=dim, keepdim=keepdim)\n",
    "\n",
    "    def norm(self, dim=None, keepdim=False, p=2, **kw) -> MCTensor:\n",
    "        return torch.norm(self, dim=dim, keepdim=keepdim, p=p)\n",
    "\n",
    "    def sinh(self) -> MCTensor:\n",
    "        return torch.sinh(self)\n",
    "    \n",
    "    def cosh(self) -> MCTensor:\n",
    "        return torch.cosh(self)\n",
    "    \n",
    "    def tanh(self) -> MCTensor:\n",
    "        return torch.tanh(self)\n",
    "\n",
    "    def atanh(self) -> MCTensor:\n",
    "        return torch.atanh(self)\n",
    "\n",
    "    def clamp_min(self, min=None) -> MCTensor:\n",
    "        return torch.clamp_min(self, min=min)\n",
    "\n",
    "    def clamp_max(self, max=None) -> MCTensor:\n",
    "        return torch.clamp_max(self, max=max)\n",
    "\n",
    "    def clone(self) -> MCTensor:\n",
    "        return torch.clone(self)\n",
    "\n",
    "    def unsqueeze(self, *args, **kwargs) -> MCTensor:\n",
    "        return torch.unsqueeze(self, *args, **kwargs)\n",
    "\n",
    "    def squeeze(self, *args, **kwargs) -> MCTensor:\n",
    "        return torch.squeeze(self, *args, **kwargs)\n",
    "\n",
    "    def reshape(self, shape) -> MCTensor:\n",
    "        if type(shape) == int:\n",
    "            shape = (shape,)\n",
    "        return torch.reshape(self, shape)\n",
    "\n",
    "    def transpose(self, *args, **kwargs) -> MCTensor:\n",
    "        return torch.transpose(self, *args, **kwargs)\n",
    "\n",
    "    def narrow(self, dim, start, length) -> MCTensor:\n",
    "        if dim < 0:\n",
    "            dim = dim + self.dim()\n",
    "        return super().narrow(dim, start, length)\n",
    "    \n",
    "    def index_select(self, dim, index) -> MCTensor:\n",
    "        if dim < 0:\n",
    "            dim = dim + self.dim()\n",
    "        return super().index_select(dim, index)\n",
    "    \n",
    "    @staticmethod\n",
    "    def replace_args(args):\n",
    "        new_args = []\n",
    "        for arg in args:\n",
    "            if isinstance(arg, list):\n",
    "                # Recursively apply the function to each element of the list\n",
    "                new_arg = MCTensor.replace_args(arg)\n",
    "            elif isinstance(arg, MCTensor):\n",
    "                # Replace MCTensor with its `res` attribute\n",
    "                new_arg = arg.res\n",
    "            else:\n",
    "                # For other types of objects, just use the original object\n",
    "                new_arg = arg\n",
    "            new_args.append(new_arg)\n",
    "        return tuple(new_args)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        # inherence nc\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        if func in HANDLED_FUNCTIONS: # torch.Tensor with MCTensor contents\n",
    "            ret = HANDLED_FUNCTIONS[func](*args, **kwargs)\n",
    "            if isinstance(ret, Tensor) and not isinstance(ret, MCTensor):\n",
    "                return cls.wrap_tensor_to_mctensor(ret)\n",
    "        else: # pytorch functions handle main and res component separately\n",
    "            ret = super().__torch_function__(func, types, args, kwargs)\n",
    "            if isinstance(ret, MCTensor) and not hasattr(ret, '_nc'):\n",
    "                new_args = cls.replace_args(args)\n",
    "                ret.res = super().__torch_function__(func, types, new_args, kwargs).as_subclass(Tensor)\n",
    "                ret._nc = ret.res.size(-1) + 1\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@implements(torch.add)\n",
    "def add(input: Union[MCTensor, int, float], other: Union[MCTensor, int, float], alpha=1) -> MCTensor:\n",
    "    if type(input) == int or type(input) == float:\n",
    "        input = torch.tensor(input, dtype=other.dtype,\n",
    "                             device=other.device)\n",
    "    if type(other) == int or type(other) == float:\n",
    "        other = torch.tensor(other, device=input.device,\n",
    "                             dtype=input.dtype)\n",
    "    if alpha != 1:\n",
    "        other = alpha * other  # should check for MCTensor multiplication\n",
    "    if isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "        return _AddMCN(input.tensor, other.tensor, simple=False)\n",
    "    elif isinstance(input, MCTensor):\n",
    "        x = input  # the MCTensor\n",
    "        value = other\n",
    "    else:\n",
    "        x = other  # the MCTensor\n",
    "        value = input\n",
    "    return _Grow_ExpN(x.tensor, value)\n",
    "\n",
    "@implements(torch.mul)\n",
    "def mul(input: Union[MCTensor, int, float], other: Union[MCTensor, int, float]) -> MCTensor:\n",
    "    if type(input) == int or type(input) == float:\n",
    "        input = torch.tensor(input, dtype=other.dtype,\n",
    "                             device=other.device)\n",
    "    if type(other) == int or type(other) == float:\n",
    "        other = torch.tensor(other, device=input.device,\n",
    "                             dtype=input.dtype)\n",
    "    normalize_case = 0 #case 0, not renormalize, explore carefully\n",
    "    if isinstance(input, MCTensor) and isinstance(other, MCTensor):\n",
    "        return _MultMCN(input.tensor, other.tensor, case=normalize_case)\n",
    "    elif isinstance(input, MCTensor):\n",
    "        x = input  # the MCTensor\n",
    "        value = other\n",
    "    else:\n",
    "        x = other  # the MCTensor\n",
    "        value = input\n",
    "    return _ScalingN(x.tensor, value)\n",
    "\n",
    "@implements(torch.div)\n",
    "def div(x: Union[MCTensor, int, float], y: Union[MCTensor, int, float]) -> MCTensor:\n",
    "    if type(x) == int or type(x) == float:\n",
    "        x = torch.tensor(x, device=y.tensor.device, dtype=y.tensor.dtype)\n",
    "    if type(y) == int or type(y) == float:\n",
    "        y = torch.tensor(y, device=x.tensor.device, dtype=x.tensor.dtype)\n",
    "    normalize_case = 2 #case 2, renormalize, explore carefully\n",
    "    if isinstance(x, MCTensor) and type(y) == Tensor:\n",
    "        nc = x.nc\n",
    "        y_tensor = torch.zeros(y.size() + (nc,), device=y.device, dtype=y.dtype)\n",
    "        y_tensor[..., 0] = y\n",
    "        result = _DivMCN(x.tensor, y_tensor, case=normalize_case)\n",
    "    elif type(x) == Tensor and isinstance(y, MCTensor):\n",
    "        nc = y.nc\n",
    "        x_tensor = torch.zeros(x.size() + (nc,), device=x.device, dtype=x.dtype)\n",
    "        x_tensor[..., 0] = x\n",
    "        result = _DivMCN(x_tensor, y.tensor, case=normalize_case)\n",
    "    elif isinstance(x, MCTensor) and isinstance(y, MCTensor):\n",
    "        result = _DivMCN(x.tensor, y.tensor, case=normalize_case)\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    return result\n",
    "\n",
    "@implements(torch.rand_like)\n",
    "def rand_like(input: MCTensor, requires_grad=False, device=None, dtype=None) -> MCTensor:\n",
    "    val = torch.randn_like(input.as_subclass(Tensor), requires_grad=requires_grad, device=device, dtype=dtype)\n",
    "    return MCTensor(val, nc=input.nc)\n",
    "\n",
    "@implements(torch.zeros_like)\n",
    "def zeros_like(input: MCTensor, requires_grad=False, device=None, dtype=None) -> MCTensor:\n",
    "    val = torch.zeros_like(input.as_subclass(Tensor), requires_grad=requires_grad, device=device, dtype=dtype)\n",
    "    return MCTensor(val, nc=input.nc)\n",
    "\n",
    "@implements(torch.ones_like)\n",
    "def ones_like(input: MCTensor, requires_grad=False, device=None, dtype=None) -> MCTensor:\n",
    "    val = torch.ones_like(input.as_subclass(Tensor), requires_grad=requires_grad, device=device, dtype=dtype)\n",
    "    return MCTensor(val, nc=input.nc)\n",
    "\n",
    "@implements(torch.clamp)\n",
    "def clamp(input: MCTensor, min=None, max=None) -> MCTensor:\n",
    "    return _clamp(input.tensor, min=min, max=max)\n",
    "\n",
    "@implements(torch.clamp_min)\n",
    "def clamp_min(input: MCTensor, min=None) -> MCTensor:\n",
    "    return _clamp(input.tensor, min=min)\n",
    "\n",
    "@implements(torch.clamp_max)\n",
    "def clamp_max(input: MCTensor, max=None) -> MCTensor:\n",
    "    return _clamp(input.tensor, max=max)\n",
    "\n",
    "@implements(torch.norm)\n",
    "def norm(input: MCTensor, dim=None, keepdim=False, p=2, **kw) -> MCTensor:\n",
    "    return _norm(input.tensor, dim=dim, keepdim=keepdim, p=p)\n",
    "\n",
    "@implements(torch.sum)\n",
    "def sum(input: MCTensor, dim=None, keepdim=False, **kw) -> MCTensor:\n",
    "    return _sum(input.tensor, dim=dim, keepdim=keepdim)\n",
    "\n",
    "@implements(torch.abs)\n",
    "def abs(input: MCTensor) -> MCTensor:\n",
    "    result_tensor = _Renormalize(input.tensor, input.nc)\n",
    "    result_tensor = torch.abs(result_tensor) ## approximation, need to change, just take neg of these with negative first componet\n",
    "    return result_tensor\n",
    "\n",
    "@implements(torch.dot)\n",
    "def dot(input: Union[MCTensor, int, float], other: Union[MCTensor, int, float]) -> MCTensor:\n",
    "    if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "        x = input\n",
    "        y = other\n",
    "    elif type(input) == Tensor and isinstance(other, MCTensor):\n",
    "        x = other\n",
    "        y = input\n",
    "    else:\n",
    "        ## implement dot product between mctensors\n",
    "        raise NotImplemented\n",
    "    return _Dot_MCN(x.tensor, y)\n",
    "\n",
    "@implements(torch.mv)\n",
    "def mv(input: Union[Tensor, MCTensor], other: Union[Tensor, MCTensor]) -> MCTensor:\n",
    "    if input.dim() == 2 and other.dim() == 1:\n",
    "        x = input  # matrix\n",
    "        y = other  # vector\n",
    "    elif input.dim() == 1 and other.dim() == 2:\n",
    "        x = other  # matrix\n",
    "        y = input  # vector\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    if isinstance(x, MCTensor) and type(y) == Tensor:\n",
    "        result = _MV_MC_T(x.tensor, y)\n",
    "    elif type(x) == Tensor and isinstance(y, MCTensor):\n",
    "        result = _MV_T_MC(x, y.tensor)\n",
    "    else:\n",
    "        ## implement mv between mctensors\n",
    "        raise NotImplemented\n",
    "    return result\n",
    "\n",
    "@implements(torch.mm)\n",
    "def mm(input: Union[Tensor, MCTensor], other: Union[Tensor, MCTensor]) -> MCTensor:\n",
    "    if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "        result = _MM_MC_T(input.tensor, other)\n",
    "    elif type(input) == Tensor and isinstance(other, MCTensor):\n",
    "        result = _MM_T_MC(input, other.tensor)\n",
    "    else:\n",
    "        ## implement mm between mctensors\n",
    "        raise NotImplemented\n",
    "    return result\n",
    "\n",
    "@implements(torch.bmm)\n",
    "def bmm(input: Union[Tensor, MCTensor], other: Union[Tensor, MCTensor]) -> MCTensor:\n",
    "    if isinstance(input, MCTensor) and type(other) == Tensor:\n",
    "        result, size, nc = _BMM_MC_T(input.tensor, other)\n",
    "    elif type(input) == Tensor and isinstance(v, MCTensor):\n",
    "        result, size, nc = _BMM_T_MC(input, other.tensor)\n",
    "    else:\n",
    "        ## implement mm between mctensors\n",
    "        raise NotImplemented\n",
    "    return result\n",
    "\n",
    "@implements(torch.matmul)\n",
    "def matmul(input: Union[MCTensor, Tensor], other: Union[MCTensor, Tensor]) -> MCTensor:\n",
    "    x_dim, y_dim = input.dim(), other.dim()\n",
    "    if x_dim == 1 and y_dim == 1:\n",
    "        return dot(input, other)\n",
    "    elif x_dim == 2 and y_dim == 2:\n",
    "        return mm(input, other)\n",
    "    elif (x_dim == 2 and y_dim == 1) or (x_dim == 1 and y_dim == 2):\n",
    "        return mv(input, other)\n",
    "    elif (x_dim > 2 and y_dim == 1) or (x_dim == 1 and y_dim > 2):\n",
    "        return mul(input, other)\n",
    "    elif x_dim == y_dim and x_dim == 3:\n",
    "        if isinstance(input, MCTensor):\n",
    "            result, size, nc = _BMM_MC_T(input.tensor, other)\n",
    "        else:\n",
    "            result, size, nc = _BMM_T_MC(input, other.tensor)\n",
    "    elif x_dim == y_dim and x_dim == 4:\n",
    "        if isinstance(input, MCTensor):\n",
    "            result, size, nc = _4DMM_MC_T(input.tensor, other)\n",
    "        else:\n",
    "            result, size, nc = _4DMM_T_MC(input, other.tensor)\n",
    "    elif x_dim > y_dim:\n",
    "        y = other[(None,) * (x_dim - y_dim)]  # unsqueeze\n",
    "        if x_dim == 3:\n",
    "            if isinstance(input, MCTensor):\n",
    "                result, size, nc = _BMM_MC_T(input.tensor, y)\n",
    "            else:\n",
    "                result, size, nc = _BMM_T_MC(input, y.tensor)\n",
    "        elif x_dim == 4:\n",
    "            if isinstance(input, MCTensor):\n",
    "                result, size, nc = _4DMM_MC_T(input.tensor, y)\n",
    "            else:\n",
    "                result, size, nc = _4DMM_T_MC(input, y.tensor)\n",
    "    elif x_dim < y_dim:\n",
    "        x = input[(None,) * (y_dim - x_dim)]  # unsqueeze\n",
    "        if y_dim == 3:\n",
    "            if isinstance(input, MCTensor):\n",
    "                result, size, nc = _BMM_MC_T(x.tensor, other)\n",
    "            else:\n",
    "                result, size, nc = _BMM_T_MC(x, other.tensor)\n",
    "        elif y_dim == 4:\n",
    "            if isinstance(input, MCTensor):\n",
    "                result, size, nc = _4DMM_MC_T(x.tensor, other)\n",
    "            else:\n",
    "                result, size, nc = _4DMM_T_MC(x, other.tensor)\n",
    "    else:\n",
    "        ## implement mm between mctensors\n",
    "        raise NotImplemented\n",
    "    return result\n",
    "\n",
    "@implements(torch.addmm)\n",
    "def addmm(input: Union[MCTensor, Tensor], \n",
    "          mat1: Union[MCTensor, Tensor], \n",
    "          mat2: Union[MCTensor, Tensor], \n",
    "          beta=1.0, alpha=1.0) -> MCTensor:\n",
    "    return beta * input + alpha * (mat1 @ mat2)\n",
    "\n",
    "@implements(torch.reshape)\n",
    "def reshape(input: MCTensor, shape) -> MCTensor:\n",
    "    data = torch.reshape(input.as_subclass(Tensor), shape)\n",
    "    extra_nc = input.res.shape[-1]\n",
    "    res = torch.reshape(input.res.view(input.res.numel() // extra_nc, extra_nc), shape + (extra_nc,))\n",
    "    return MCTensor.wrap_tensor_and_res_to_mctensor(data, res)\n",
    "\n",
    "@implements(torch.nn.functional.relu)\n",
    "def relu(input: MCTensor, inplace=False) -> MCTensor:\n",
    "    val = torch.nn.functional.relu(input.as_subclass(Tensor), inplace=inplace)\n",
    "    if inplace:\n",
    "        input.res[input.as_subclass(Tensor) == 0] = 0\n",
    "        return input\n",
    "    else:\n",
    "        res = input.res.clone()\n",
    "        res[val == 0] = 0\n",
    "        return MCTensor.wrap_tensor_and_res_to_mctensor(val, res)\n",
    "    \n",
    "@implements(torch.sigmoid)\n",
    "def sigmoid(input) -> MCTensor:\n",
    "    return 1/(torch.exp(-input) + 1)\n",
    "\n",
    "@implements(torch.nn.functional.softmax)\n",
    "def softmax(x: MCTensor, dim, *args, **kwargs) -> MCTensor:\n",
    "    return _softmax(x.tensor, dim=dim)\n",
    "\n",
    "@implements(torch.erf)\n",
    "def erf(input: MCTensor) -> MCTensor:\n",
    "    ### this is an approximation\n",
    "    ret = torch.erf(input.as_subclass(Tensor))\n",
    "    return MCTensor(ret, nc=input.nc)\n",
    "\n",
    "@implements(torch.nn.functional.dropout)\n",
    "def dropout(input: MCTensor, p=0.5, training=True, inplace=False) -> MCTensor:\n",
    "    if training:\n",
    "        val = torch.nn.functional.dropout(input.as_subclass(Tensor), p=p, training=True, inplace=inplace)\n",
    "        if inplace:\n",
    "            input.res[input.as_subclass(Tensor) == 0] = 0\n",
    "            return input\n",
    "        else:\n",
    "            res = input.res.clone()\n",
    "            res[val == 0] = 0\n",
    "            return MCTensor.wrap_tensor_and_res_to_mctensor(val, res)    \n",
    "    else:\n",
    "        return input\n",
    "    \n",
    "@implements(torch.square)\n",
    "def square(input: MCTensor) -> MCTensor:\n",
    "    return _square(input.tensor)\n",
    "\n",
    "@implements(torch.atanh)\n",
    "def atanh(input: MCTensor) -> MCTensor:\n",
    "    return _atanh(input.tensor)\n",
    "\n",
    "@implements(torch.log1p)\n",
    "def log1p(input: MCTensor) -> MCTensor:\n",
    "    return _log1p_standard(input.tensor)\n",
    "\n",
    "@implements(torch.nn.functional.linear)\n",
    "def linear(input: Union[MCTensor, Tensor], weight: Union[MCTensor, Tensor], bias=None) -> MCTensor:\n",
    "    if isinstance(input, MCTensor) and isinstance(weight, MCTensor):\n",
    "        ## attention, here make input as tensor, as mul between MCTensors not supported yet\n",
    "        input = input.as_subclass(Tensor)\n",
    "    ret = torch.matmul(input, weight.T)\n",
    "    if bias is None:\n",
    "        return ret\n",
    "    else:\n",
    "        return ret + bias\n",
    "    \n",
    "@implements(torch.diag)\n",
    "def diag(x: MCTensor, diagonal=0) -> MCTensor:\n",
    "    indices_selected = torch.arange(x.numel(), dtype=torch.int64, device=x.device).view(*x.shape)\n",
    "    selected_indices = torch.diag(indices_selected, diagonal=diagonal).view(-1)\n",
    "    val = x.as_subclass(Tensor).view(-1)[selected_indices]\n",
    "    res = x.res.view(x.numel(), x.res.shape[-1])[selected_indices]\n",
    "    return MCTensor.wrap_tensor_and_res_to_mctensor(val, res)\n",
    "\n",
    "@implements(torch.mean)\n",
    "def mean(input: MCTensor, dim=None, keepdim=False, **kw) -> MCTensor:\n",
    "    return _mean(input.tensor, dim=dim, keepdim=keepdim)\n",
    "\n",
    "@implements(torch.nn.functional.nll_loss)\n",
    "def nll_loss(input: MCTensor, target: Tensor, **kw) -> MCTensor:\n",
    "    return torch.mean(torch.diag(-input[:, target]))\n",
    "\n",
    "@implements(torch.nn.functional.log_softmax)\n",
    "def log_softmax(x: MCTensor, dim=None, **kw) -> MCTensor:\n",
    "    return _log_softmax(x.tensor, dim=dim)\n",
    "\n",
    "@implements(torch.nn.functional.cross_entropy)\n",
    "def cross_entropy(x: MCTensor, target: Tensor, reduction='mean', label_smoothing=0.0, **kw) -> MCTensor:\n",
    "    cross_entropy_x_tensor = _cross_entropy(x.tensor, target=target, reduction=reduction, label_smoothing=label_smoothing)\n",
    "    return cross_entropy_x_tensor\n",
    "\n",
    "@implements(torch.nn.functional.mse_loss)\n",
    "def mse_loss(x: MCTensor, y: MCTensor, reduction='mean', **kw) -> MCTensor:\n",
    "    return _mse_loss(x.tensor, y.tensor, reduction=reduction)\n",
    "\n",
    "@implements(torch.sqrt)\n",
    "def sqrt(input: MCTensor) -> MCTensor:\n",
    "    return _sqrt(input.tensor)\n",
    "\n",
    "@implements(torch.log)\n",
    "def log(input: MCTensor) -> MCTensor:\n",
    "    return _log(input.tensor)\n",
    "\n",
    "@implements(torch.pow)\n",
    "def pow(input: MCTensor, exponent: Union[Tensor, float, int]) -> MCTensor:\n",
    "    return  _pow(input.tensor, exponent)\n",
    "\n",
    "@implements(torch.exp)\n",
    "def exp(input: MCTensor) -> MCTensor:\n",
    "    return _exp(input.tensor)\n",
    "\n",
    "@implements(torch.sinh)\n",
    "def sinh(input: MCTensor) -> MCTensor:\n",
    "    return _sinh(input.tensor)\n",
    "\n",
    "@implements(torch.cosh)\n",
    "def cosh(input: MCTensor) -> MCTensor:\n",
    "    return _cosh(input.tensor)\n",
    "\n",
    "@implements(torch.tanh)\n",
    "def tanh(input: MCTensor) -> MCTensor:\n",
    "    return _tanh(input.tensor)\n",
    "\n",
    "@implements(torch.nn.functional.layer_norm)\n",
    "def layer_norm(x: MCTensor, normalized_shape, weight=None, bias=None, eps=1e-05) -> MCTensor:\n",
    "    nc = x.nc\n",
    "    if isinstance(weight, torch.Tensor):\n",
    "        mc_weight = torch.zeros(weight.size() + (nc,),\n",
    "                                device=x.device, dtype=x.dtype)\n",
    "        mc_weight[..., 0] = weight\n",
    "    else:\n",
    "        mc_weight = weight.tensor\n",
    "\n",
    "    if isinstance(bias, torch.Tensor):\n",
    "        mc_bias = torch.zeros(bias.size() + (nc,),\n",
    "                              device=x.device, dtype=x.dtype)\n",
    "        mc_bias[..., 0] = bias\n",
    "    else:\n",
    "        mc_bias = bias.tensor\n",
    "    return _layer_norm(x.tensor, normalized_shape, mc_weight, mc_bias, eps=eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDos and attentions:\n",
    "1. check each (customized) function is really called and working instead of super torch class functions\n",
    "2. fix torch.abs to be correct, i.e., make it a neg for those with negative first component\n",
    "3. support torch.norm with p='fro' Frobenius norm\n",
    "4. check renormalize, i.e., case number (0,1,2) and simple (True,False) in add, mul, div, etc., specifically when they use lower level functions such as _AddMCN, _ScalingN ...\n",
    "5. implement MCTensor multiplication with MCTensor at high level, i.e., dot, mv, mm etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2, 3, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = MCTensor([[0.1, 0.3, 0.2], [0.2, 0.4, 0.5]], nc=2)\n",
    "a.res.add_(-0.001)\n",
    "a.shape, a.res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.as_subclass(Tensor).data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -0.0010],\n",
       "         [ 0.0000, -0.0010],\n",
       "         [ 0.0000, -0.0010]],\n",
       "\n",
       "        [[ 0.0000, -0.0010],\n",
       "         [ 0.0000, -0.0010],\n",
       "         [ 0.0000, -0.0010]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MCTensor([[0.1000, 0.3000, 0.2000],\n",
       "           [0.2000, 0.4000, 0.5000]]), nc=2,\n",
       " tensor([[[-0.0010],\n",
       "          [-0.0010],\n",
       "          [-0.0010]],\n",
       " \n",
       "         [[-0.0010],\n",
       "          [-0.0010],\n",
       "          [-0.0010]]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, a.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.9581e-09],\n",
       "         [-2.5844e-08],\n",
       "         [ 3.9581e-09]],\n",
       "\n",
       "        [[ 3.9581e-09],\n",
       "         [-2.5844e-08],\n",
       "         [-2.5844e-08]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a + a).res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCTensor([[1.0990, 1.2990, 1.1990],\n",
       "          [1.1990, 1.3990, 1.4990]]), nc=2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCTensor([[0.9010, 0.7010, 0.8010],\n",
       "          [0.8010, 0.6010, 0.5010]]), nc=2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.0 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 1, 1]), 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.index_select(-1, torch.tensor(1))\n",
    "b.shape, b.res.shape, b.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MCTensor([[0.3000],\n",
       "           [0.4000]]), nc=2,\n",
       " tensor([[[-0.0010]],\n",
       " \n",
       "         [[-0.0010]]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, b.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0010])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(a).res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter __neg__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MCTensor(-0.1250), nc=2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.diag(-a[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc = torch.nn.functional.nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter __neg__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MCTensor(-0.1500), nc=2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfunc(a, torch.tensor([0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 2]), torch.Size([4, 2, 1]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.cat([a,a])\n",
    "b.shape, b.res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4]) torch.Size([2, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "c = torch.reshape(b, (2, 4))\n",
    "print(c.shape, c.res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MCTensor([[0.1000, 0.3000, 0.2000, 0.4000],\n",
       "           [0.1000, 0.3000, 0.2000, 0.4000]]), nc=2,\n",
       " tensor([[[-0.0010],\n",
       "          [-0.0010],\n",
       "          [-0.0010],\n",
       "          [-0.0010]],\n",
       " \n",
       "         [[-0.0010],\n",
       "          [-0.0010],\n",
       "          [-0.0010],\n",
       "          [-0.0010]]]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c, c.res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy-torch",
   "language": "python",
   "name": "hy-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
