{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTorch hyperbolic embedding for the WordNet Mammals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import torch, HTorch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import logging\n",
    "from hype.graph import load_edge_list, eval_reconstruction\n",
    "from HTorch.layers import HEmbedding\n",
    "from HTorch.optimizers import RiemannianSGD, RiemannianAdam\n",
    "import sys, os, random\n",
    "import json\n",
    "import torch.multiprocessing as mp\n",
    "from hype.graph_dataset import BatchedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils, matutils  # utility fnc for pickling, common scipy operations etc\n",
    "from six import iteritems, itervalues, string_types\n",
    "from six.moves import xrange\n",
    "from numpy import dot, zeros, dtype, float32 as REAL,\\\n",
    "    double, array, vstack, fromstring, sqrt, newaxis,\\\n",
    "    ndarray, sum as np_sum, prod, ascontiguousarray,\\\n",
    "    argmax\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    \"\"\"\n",
    "    A single vocabulary item, used internally for collecting per-word frequency/sampling info,\n",
    "    and for constructing binary trees (incl. both word leaves and inner nodes).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.count = 0\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __lt__(self, other):  # used for sorting in a priority queue\n",
    "        return self.count < other.count\n",
    "\n",
    "    def __str__(self):\n",
    "        vals = ['%s:%r' % (key, self.__dict__[key]) for key in sorted(self.__dict__) if not key.startswith('_')]\n",
    "        return \"%s(%s)\" % (self.__class__.__name__, ', '.join(vals))\n",
    "\n",
    "\n",
    "class KeyedVectorsBase(utils.SaveLoad):\n",
    "    \"\"\"\n",
    "    Base class to contain vectors and vocab for any set of vectors which are each associated with a key.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.syn0 = []\n",
    "        self.vocab = {}\n",
    "        self.index2word = []\n",
    "        self.vector_size = None\n",
    "\n",
    "    def save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None):\n",
    "        \"\"\"\n",
    "        Store the input-hidden weight matrix in the same format used by the original\n",
    "        C word2vec-tool, for compatibility.\n",
    "         `fname` is the file used to save the vectors in\n",
    "         `fvocab` is an optional file used to save the vocabulary\n",
    "         `binary` is an optional boolean indicating whether the data is to be saved\n",
    "         in binary word2vec format (default: False)\n",
    "         `total_vec` is an optional parameter to explicitly specify total no. of vectors\n",
    "         (in case word vectors are appended with document vectors afterwards)\n",
    "        \"\"\"\n",
    "        if total_vec is None:\n",
    "            total_vec = len(self.vocab)\n",
    "        vector_size = self.syn0.shape[1]\n",
    "        if fvocab is not None:\n",
    "            logger.info(\"storing vocabulary in %s\", fvocab)\n",
    "            with utils.smart_open(fvocab, 'wb') as vout:\n",
    "                for word, vocab in sorted(iteritems(self.vocab), key=lambda item: -item[1].count):\n",
    "                    vout.write(utils.to_utf8(\"%s %s\\n\" % (word, vocab.count)))\n",
    "        logger.info(\"storing %sx%s projection weights into %s\", total_vec, vector_size, fname)\n",
    "        assert (len(self.vocab), vector_size) == self.syn0.shape\n",
    "        with utils.smart_open(fname, 'wb') as fout:\n",
    "            fout.write(utils.to_utf8(\"%s %s\\n\" % (total_vec, vector_size)))\n",
    "            # store in sorted order: most frequent words at the top\n",
    "            for word, vocab in sorted(iteritems(self.vocab), key=lambda item: -item[1].count):\n",
    "                row = self.syn0[vocab.index]\n",
    "                if binary:\n",
    "                    fout.write(utils.to_utf8(word) + b\" \" + row.tostring())\n",
    "                else:\n",
    "                    fout.write(utils.to_utf8(\"%s %s\\n\" % (word, ' '.join(\"%f\" % val for val in row))))\n",
    "\n",
    "    @classmethod\n",
    "    def load_word2vec_format(cls, fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict',\n",
    "                             limit=None, datatype=REAL):\n",
    "        \"\"\"\n",
    "        Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
    "        Note that the information stored in the file is incomplete (the binary tree is missing),\n",
    "        so while you can query for word similarity etc., you cannot continue training\n",
    "        with a model loaded this way.\n",
    "        `binary` is a boolean indicating whether the data is in binary word2vec format.\n",
    "        `norm_only` is a boolean indicating whether to only store normalised word2vec vectors in memory.\n",
    "        Word counts are read from `fvocab` filename, if set (this is the file generated\n",
    "        by `-save-vocab` flag of the original C tool).\n",
    "        If you trained the C model using non-utf8 encoding for words, specify that\n",
    "        encoding in `encoding`.\n",
    "        `unicode_errors`, default 'strict', is a string suitable to be passed as the `errors`\n",
    "        argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
    "        file may include word tokens truncated in the middle of a multibyte unicode character\n",
    "        (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
    "        `limit` sets a maximum number of word-vectors to read from the file. The default,\n",
    "        None, means read all.\n",
    "        `datatype` (experimental) can coerce dimensions to a non-default float type (such\n",
    "        as np.float16) to save memory. (Such types may result in much slower bulk operations\n",
    "        or incompatibility with optimized routines.)\n",
    "        \"\"\"\n",
    "        counts = None\n",
    "        if fvocab is not None:\n",
    "            logger.info(\"loading word counts from %s\", fvocab)\n",
    "            counts = {}\n",
    "            with utils.smart_open(fvocab) as fin:\n",
    "                for line in fin:\n",
    "                    word, count = utils.to_unicode(line).strip().split()\n",
    "                    counts[word] = int(count)\n",
    "\n",
    "        logger.info(\"loading projection weights from %s\", fname)\n",
    "        with utils.smart_open(fname) as fin:\n",
    "            header = utils.to_unicode(fin.readline(), encoding=encoding)\n",
    "            vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format\n",
    "            if limit:\n",
    "                vocab_size = min(vocab_size, limit)\n",
    "            result = cls()\n",
    "            result.vector_size = vector_size\n",
    "            result.syn0 = zeros((vocab_size, vector_size), dtype=datatype)\n",
    "\n",
    "            def add_word(word, weights):\n",
    "                word_id = len(result.vocab)\n",
    "                if word in result.vocab:\n",
    "                    logger.warning(\"duplicate word '%s' in %s, ignoring all but first\", word, fname)\n",
    "                    return\n",
    "                if counts is None:\n",
    "                    # most common scenario: no vocab file given. just make up some bogus counts, in descending order\n",
    "                    result.vocab[word] = Vocab(index=word_id, count=vocab_size - word_id)\n",
    "                elif word in counts:\n",
    "                    # use count from the vocab file\n",
    "                    result.vocab[word] = Vocab(index=word_id, count=counts[word])\n",
    "                else:\n",
    "                    # vocab file given, but word is missing -- set count to None (TODO: or raise?)\n",
    "                    logger.warning(\"vocabulary file is incomplete: '%s' is missing\", word)\n",
    "                    result.vocab[word] = Vocab(index=word_id, count=None)\n",
    "                result.syn0[word_id] = weights\n",
    "                result.index2word.append(word)\n",
    "\n",
    "            if binary:\n",
    "                binary_len = dtype(REAL).itemsize * vector_size\n",
    "                for _ in xrange(vocab_size):\n",
    "                    # mixed text and binary: read text first, then binary\n",
    "                    word = []\n",
    "                    while True:\n",
    "                        ch = fin.read(1)\n",
    "                        if ch == b' ':\n",
    "                            break\n",
    "                        if ch == b'':\n",
    "                            raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n",
    "                        if ch != b'\\n':  # ignore newlines in front of words (some binary files have)\n",
    "                            word.append(ch)\n",
    "                    word = utils.to_unicode(b''.join(word), encoding=encoding, errors=unicode_errors)\n",
    "                    weights = fromstring(fin.read(binary_len), dtype=REAL)\n",
    "                    add_word(word, weights)\n",
    "            else:\n",
    "                for line_no in xrange(vocab_size):\n",
    "                    line = fin.readline()\n",
    "                    if line == b'':\n",
    "                        raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n",
    "                    parts = utils.to_unicode(line.rstrip(), encoding=encoding, errors=unicode_errors).split(\" \")\n",
    "                    if len(parts) != vector_size + 1:\n",
    "                        raise ValueError(\"invalid vector on line %s (is this really the text format?)\" % line_no)\n",
    "                    word, weights = parts[0], [REAL(x) for x in parts[1:]]\n",
    "                    add_word(word, weights)\n",
    "        if result.syn0.shape[0] != len(result.vocab):\n",
    "            logger.info(\n",
    "                \"duplicate words detected, shrinking matrix size from %i to %i\",\n",
    "                result.syn0.shape[0], len(result.vocab)\n",
    "            )\n",
    "            result.syn0 = ascontiguousarray(result.syn0[: len(result.vocab)])\n",
    "        assert (len(result.vocab), vector_size) == result.syn0.shape\n",
    "\n",
    "        logger.info(\"loaded %s matrix from %s\", result.syn0.shape, fname)\n",
    "        return result\n",
    "\n",
    "    def similarity(self, w1, w2):\n",
    "        \"\"\"\n",
    "        Compute similarity between vectors of two input words.\n",
    "        To be implemented by child class.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def distance(self, w1, w2):\n",
    "        \"\"\"\n",
    "        Compute distance between vectors of two input words.\n",
    "        To be implemented by child class.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def distances(self, word_or_vector, other_words=()):\n",
    "        \"\"\"\n",
    "        Compute distances from given word or vector to all words in `other_words`.\n",
    "        If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
    "        To be implemented by child class.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def word_vec(self, word):\n",
    "        \"\"\"\n",
    "        Accept a single word as input.\n",
    "        Returns the word's representations in vector space, as a 1D numpy array.\n",
    "        Example::\n",
    "          >>> trained_model.word_vec('office')\n",
    "          array([ -1.40128313e-02, ...])\n",
    "        \"\"\"\n",
    "        if word in self.vocab:\n",
    "            result = self.syn0[self.vocab[word].index]\n",
    "            result.setflags(write=False)\n",
    "            return result\n",
    "        else:\n",
    "            raise KeyError(\"word '%s' not in vocabulary\" % word)\n",
    "\n",
    "    def __getitem__(self, words):\n",
    "        \"\"\"\n",
    "        Accept a single word or a list of words as input.\n",
    "        If a single word: returns the word's representations in vector space, as\n",
    "        a 1D numpy array.\n",
    "        Multiple words: return the words' representations in vector space, as a\n",
    "        2d numpy array: #words x #vector_size. Matrix rows are in the same order\n",
    "        as in input.\n",
    "        Example::\n",
    "          >>> trained_model['office']\n",
    "          array([ -1.40128313e-02, ...])\n",
    "          >>> trained_model[['office', 'products']]\n",
    "          array([ -1.40128313e-02, ...]\n",
    "                [ -1.70425311e-03, ...]\n",
    "                 ...)\n",
    "        \"\"\"\n",
    "        if isinstance(words, string_types):\n",
    "            # allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\n",
    "            return self.word_vec(words)\n",
    "\n",
    "        return vstack([self.word_vec(word) for word in words])\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        return word in self.vocab\n",
    "\n",
    "    def most_similar_to_given(self, w1, word_list):\n",
    "        \"\"\"Return the word from word_list most similar to w1.\n",
    "        Args:\n",
    "            w1 (str): a word\n",
    "            word_list (list): list of words containing a word most similar to w1\n",
    "        Returns:\n",
    "            the word in word_list with the highest similarity to w1\n",
    "        Raises:\n",
    "            KeyError: If w1 or any word in word_list is not in the vocabulary\n",
    "        Example::\n",
    "          >>> trained_model.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "          'sound'\n",
    "          >>> trained_model.most_similar_to_given('snake', ['food', 'pencil', 'animal', 'phone'])\n",
    "          'animal'\n",
    "        \"\"\"\n",
    "        return word_list[argmax([self.similarity(w1, word) for word in word_list])]\n",
    "\n",
    "    def words_closer_than(self, w1, w2):\n",
    "        \"\"\"\n",
    "        Returns all words that are closer to `w1` than `w2` is to `w1`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        w1 : str\n",
    "            Input word.\n",
    "        w2 : str\n",
    "            Input word.\n",
    "        Returns\n",
    "        -------\n",
    "        list (str)\n",
    "            List of words that are closer to `w1` than `w2` is to `w1`.\n",
    "        Examples\n",
    "        --------\n",
    "        >>> model.words_closer_than('carnivore.n.01', 'mammal.n.01')\n",
    "        ['dog.n.01', 'canine.n.02']\n",
    "        \"\"\"\n",
    "        all_distances = self.distances(w1)\n",
    "        w1_index = self.vocab[w1].index\n",
    "        w2_index = self.vocab[w2].index\n",
    "        closer_node_indices = np.where(all_distances < all_distances[w2_index])[0]\n",
    "        return [self.index2word[index] for index in closer_node_indices if index != w1_index]\n",
    "\n",
    "    def rank(self, w1, w2):\n",
    "        \"\"\"\n",
    "        Rank of the distance of `w2` from `w1`, in relation to distances of all words from `w1`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        w1 : str\n",
    "            Input word.\n",
    "        w2 : str\n",
    "            Input word.\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            Rank of `w2` from `w1` in relation to all other nodes.\n",
    "        Examples\n",
    "        --------\n",
    "        >>> model.rank('mammal.n.01', 'carnivore.n.01')\n",
    "        3\n",
    "        \"\"\"\n",
    "        return len(self.words_closer_than(w1, w2)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAGEmbeddingKeyedVectors(KeyedVectorsBase):\n",
    "    \"\"\"Class to contain vectors and vocab for the\n",
    "     :class:`~gensim.models.poincare.DAGEmbeddingKeyedVectorsModel` training class.\n",
    "        Used to perform operations on the vectors such as vector lookup, distance etc.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DAGEmbeddingKeyedVectors, self).__init__()\n",
    "        self.syn0 = []\n",
    "\n",
    "    def vector_distance(self, vector_1, vector_2):\n",
    "        \"\"\"\n",
    "        Return poincare distance between two input vectors. Convenience method over `vector_distance_batch`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        vector_1 : numpy.array\n",
    "            input vector\n",
    "        vector_2 : numpy.array\n",
    "            input vector\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.float\n",
    "            Distance between `vector_1` and `vector_2`.\n",
    "        \"\"\"\n",
    "        return DAGEmbeddingKeyedVectors.vector_distance_batch(vector_1, vector_2[np.newaxis, :])[0]\n",
    "\n",
    "\n",
    "    def distance(self, w1, w2):\n",
    "        \"\"\"\n",
    "        Return distance between vectors for nodes `w1` and `w2`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        w1 : str or int\n",
    "            Key for first node.\n",
    "        w2 : str or int\n",
    "            Key for second node.\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            distance between the vectors for nodes `w1` and `w2`.\n",
    "        Examples\n",
    "        --------\n",
    "        >>> model.distance('mammal.n.01', 'carnivore.n.01')\n",
    "        2.13\n",
    "        Notes\n",
    "        -----\n",
    "        Raises KeyError if either of `w1` and `w2` is absent from vocab.\n",
    "        \"\"\"\n",
    "        vector_1 = self.word_vec(w1)\n",
    "        vector_2 = self.word_vec(w2)\n",
    "        return self.vector_distance(vector_1, vector_2)\n",
    "\n",
    "\n",
    "    def most_similar(self, node_or_vector, topn=10, restrict_vocab=None):\n",
    "        \"\"\"\n",
    "        Find the top-N most similar nodes to the given node or vector, sorted in increasing order of distance.\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_or_vector : str/int or numpy.array\n",
    "            node key or vector for which similar nodes are to be found.\n",
    "        topn : int or None, optional\n",
    "            number of similar nodes to return, if `None`, returns all.\n",
    "        restrict_vocab : int or None, optional\n",
    "            Optional integer which limits the range of vectors which are searched for most-similar values.\n",
    "            For example, restrict_vocab=10000 would only check the first 10000 node vectors in the vocabulary order.\n",
    "            This may be meaningful if vocabulary is sorted by descending frequency.\n",
    "        Returns\n",
    "        --------\n",
    "        list of tuples (str, float)\n",
    "            List of tuples containing (node, distance) pairs in increasing order of distance.\n",
    "        Examples\n",
    "        --------\n",
    "        >>> vectors.most_similar('lion.n.01')\n",
    "        [('lion_cub.n.01', 0.4484), ('lionet.n.01', 0.6552), ...]\n",
    "        \"\"\"\n",
    "        if not restrict_vocab:\n",
    "            all_distances = self.distances(node_or_vector)\n",
    "        else:\n",
    "            nodes_to_use = self.index2word[:restrict_vocab]\n",
    "            all_distances = self.distances(node_or_vector, nodes_to_use)\n",
    "\n",
    "        if isinstance(node_or_vector, string_types + (int,)):\n",
    "            node_index = self.vocab[node_or_vector].index\n",
    "        else:\n",
    "            node_index = None\n",
    "        if not topn:\n",
    "            closest_indices = matutils.argsort(all_distances)\n",
    "        else:\n",
    "            closest_indices = matutils.argsort(all_distances, topn=1 + topn)\n",
    "        result = [\n",
    "            (self.index2word[index], float(all_distances[index]))\n",
    "            for index in closest_indices if (not node_index or index != node_index)  # ignore the input node\n",
    "        ]\n",
    "        if topn:\n",
    "            result = result[:topn]\n",
    "        return result\n",
    "\n",
    "\n",
    "    def vector_distance_batch(self, vector_1, vectors_all):\n",
    "        \"\"\"\n",
    "        Return distances between one vector and a set of other vectors.\n",
    "        Parameters\n",
    "        ----------\n",
    "        vector_1 : numpy.array\n",
    "            vector from which distances are to be computed.\n",
    "            expected shape (dim,)\n",
    "        vectors_all : numpy.array\n",
    "            for each row in vectors_all, distance from vector_1 is computed.\n",
    "            expected shape (num_vectors, dim)\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Contains distance between vector_1 and each row in vectors_all.\n",
    "            shape (num_vectors,)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def distances_from_indices(self, node_index, other_indices=()):\n",
    "        assert node_index < len(self.syn0)\n",
    "        input_vector = self.syn0[node_index]\n",
    "        if not other_indices:\n",
    "            other_vectors = self.syn0\n",
    "        else:\n",
    "            other_vectors = self.syn0[other_indices]\n",
    "        return self.vector_distance_batch(input_vector, other_vectors)\n",
    "\n",
    "\n",
    "    def is_a_scores_vector_batch(self, alpha, parent_vectors, other_vectors, rel_reversed):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def is_a_scores_from_indices(self, alpha, parent_indices, other_indices, rel_reversed):\n",
    "        parent_vectors = self.syn0[parent_indices]\n",
    "        other_vectors = self.syn0[other_indices]\n",
    "        return self.is_a_scores_vector_batch(alpha, parent_vectors, other_vectors, rel_reversed)\n",
    "\n",
    "\n",
    "    def distances(self, node_or_vector, other_nodes=()):\n",
    "        \"\"\"\n",
    "        Compute distances from given node or vector to all nodes in `other_nodes`.\n",
    "        If `other_nodes` is empty, return distance between `node_or_vector` and all nodes in vocab.\n",
    "        Parameters\n",
    "        ----------\n",
    "        node_or_vector : str/int or numpy.array\n",
    "            Node key or vector from which distances are to be computed.\n",
    "        other_nodes : iterable of str/int or None\n",
    "            For each node in `other_nodes` distance from `node_or_vector` is computed.\n",
    "            If None or empty, distance of `node_or_vector` from all nodes in vocab is computed (including itself).\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Array containing distances to all nodes in `other_nodes` from input `node_or_vector`,\n",
    "            in the same order as `other_nodes`.\n",
    "        Examples\n",
    "        --------\n",
    "        >>> model.distances('mammal.n.01', ['carnivore.n.01', 'dog.n.01'])\n",
    "        np.array([2.1199, 2.0710]\n",
    "        >>> model.distances('mammal.n.01')\n",
    "        np.array([0.43753847, 3.67973852, ..., 6.66172886])\n",
    "        Notes\n",
    "        -----\n",
    "        Raises KeyError if either `node_or_vector` or any node in `other_nodes` is absent from vocab.\n",
    "        \"\"\"\n",
    "        if isinstance(node_or_vector, string_types):\n",
    "            input_vector = self.word_vec(node_or_vector)\n",
    "        else:\n",
    "            input_vector = node_or_vector\n",
    "        if other_nodes == None:\n",
    "            other_vectors = self.syn0\n",
    "        else:\n",
    "            other_indices = [self.vocab[node].index for node in other_nodes]\n",
    "            other_vectors = self.syn0[other_indices]\n",
    "        return self.vector_distance_batch(input_vector, other_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model defined using HTorch\n",
    "class EnergyFunction(torch.nn.Module):\n",
    "    def __init__(self, size, dim, sparse=False, manifold='PoincareBall', curvature=-1.0, **kwargs):\n",
    "        super().__init__()\n",
    "        # initialize layer, weights are automatically initialized around origin\n",
    "        self.lt = HEmbedding(size, dim, sparse=sparse, manifold=manifold, curvature=curvature) \n",
    "        self.nobjects = size\n",
    "        self.kv = DAGEmbeddingKeyedVectors\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        e = self.lt(inputs)\n",
    "        with torch.no_grad():\n",
    "            e.proj_()\n",
    "        o = e.narrow(1, 1, e.size(1) - 1)\n",
    "        s = e.narrow(1, 0, 1).expand_as(o)\n",
    "        return o.Hdist(s).squeeze(-1)\n",
    "    \n",
    "    def loss(self, inp, target, **kwargs):\n",
    "        return F.cross_entropy(inp.neg(), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set meta-parameters, float precision etc.\n",
    "os.environ[\"NUMEXPR_MAX_THREADS\"] = '8'\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# note d16, d32 may produce infs and NaNs due to imprecision\n",
    "d16 = torch.float16; d32 = torch.float32; d64 = torch.float64\n",
    "cpu = torch.device(\"cpu\"); gpu = torch.device(type='cuda', index=0)\n",
    "device = cpu\n",
    "opt_dtype = d64\n",
    "\n",
    "if opt_dtype == d16:\n",
    "    dtype = \"d16\"\n",
    "    torch.set_default_tensor_type('torch.HalfTensor')\n",
    "elif opt_dtype == d32:\n",
    "    dtype = \"d32\"\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "else:\n",
    "    dtype = \"d64\"\n",
    "    torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for PyTorch Poincare Halfspace model: (now use opt_epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters; these are global in the notebook!\n",
    "opt_maxnorm = 500000; opt_debug = False;\n",
    "opt_dim = 2; #for Lorentz, add 1 to dim relative to Poincare/HalfSpace\n",
    "opt_negs = 50;  opt_eval_each = 20;\n",
    "opt_sparse = True; opt_ndproc = 1;  opt_burnin = 20;\n",
    "opt_dampening = 0.75; opt_neg_multiplier = 1.0; \n",
    "opt_burnin_multiplier = 0.01; \n",
    "###########################################################\n",
    "opt_epochs = 10; opt_batchsize = 32; \n",
    "opt_lr = 1.0;  opt_dscale = 1.0\n",
    "#opt_manifold = \"PoincareBall\"\n",
    "# opt_manifold = \"Lorentz\"\n",
    "opt_manifold = \"HalfSpace\"\n",
    "opt_curvature = -1.0 \n",
    "opt_task = 'mammals'\n",
    "#######################################\n",
    "FILE_NAME = \"_\".join([opt_task, 'lr', str(opt_lr), 'batch', str(opt_batchsize),\n",
    "                      str(opt_epochs), \"torch\", dtype, str(opt_dscale)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing logging and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using edge list dataloader\n"
     ]
    }
   ],
   "source": [
    "log_level = logging.DEBUG if opt_debug else logging.INFO\n",
    "log = logging.getLogger('Embedding')\n",
    "logging.basicConfig(level=log_level, format='%(message)s', stream=sys.stdout)\n",
    "log.info('Using edge list dataloader')\n",
    "idx, objects, weights = load_edge_list(\"wordnet/mammal_closure.csv\", False) \n",
    "#idx, objects, weights = load_edge_list(\"/home/jl3789/Hyperbolic_Library/applications/poincare_embedding/wordnet/mammal_closure.csv\", False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(manifold, curvature, dim, idx, objects, weights, sparse=True):\n",
    "    model_name = '%s_dim%d'\n",
    "    mname = model_name % (manifold, dim)\n",
    "    data = BatchedDataset(idx, objects, weights, opt_negs, opt_batchsize,\n",
    "        opt_ndproc, opt_burnin > 0, opt_dampening)\n",
    "    model = EnergyFunction(len(data.objects), opt_dim, sparse=sparse, manifold=manifold, curvature=curvature)\n",
    "    data.objects = objects\n",
    "    return model, data, mname\n",
    "\n",
    "def adj_matrix(data):\n",
    "    adj = {}\n",
    "    for inputs, _ in data:\n",
    "        for row in inputs:\n",
    "            x = row[0].item()\n",
    "            y = row[1].item()\n",
    "            if x in adj:\n",
    "                adj[x].add(y)\n",
    "            else:\n",
    "                adj[x] = {y}\n",
    "    return adj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import logging\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def space_title(s):\n",
    "    for cut in [100, 200, 300, 400]:\n",
    "        if len(s) > cut:\n",
    "            x = s.find(';', cut) + 1\n",
    "            if x > 0:\n",
    "                s = s[:x] + '<br>' + s[x:]\n",
    "    return s\n",
    "\n",
    "def create_animation(figure_title):\n",
    "    figure = {'data': [],\n",
    "              'layout': {'xaxis': {'range': [-1, 1.3], 'autorange': False, 'zeroline' :False, 'showgrid' :False},\n",
    "                         'yaxis': {'range': [-1, 1.3], 'autorange': False, 'zeroline' :False, 'showgrid' :False},\n",
    "                         'title': space_title(figure_title),\n",
    "                         'width': 1200,\n",
    "                         'height': 1200,\n",
    "                         'showlegend': False,\n",
    "                         'hovermode': 'closest',\n",
    "                         'updatemenus': [{\n",
    "                             'type': 'buttons',\n",
    "                             'buttons': [\n",
    "                                 {'label': 'Play',\n",
    "                                  'method': 'animate',\n",
    "                                  'args': [None]},\n",
    "                                 {\n",
    "                                     'args': [[None], {'frame': {'duration': 0, 'redraw': False},\n",
    "                                                       'mode': 'immediate',\n",
    "                                                       'transition': {'duration': 0}}],\n",
    "                                     'label': 'Pause',\n",
    "                                     'method': 'animate'\n",
    "                                 }\n",
    "                             ]}]\n",
    "                         },\n",
    "              'frames': []}\n",
    "    return figure\n",
    "\n",
    "\n",
    "def poincare_2d_visualization(\n",
    "        model,\n",
    "        animation,\n",
    "        epoch,\n",
    "        eval_result,\n",
    "        avg_loss,\n",
    "        avg_pos_loss,\n",
    "        avg_neg_loss,\n",
    "        tree,\n",
    "        figure_title,\n",
    "        num_nodes=50,\n",
    "        show_node_labels=()):\n",
    "    \"\"\"Create a 2-d plot of the nodes and edges of a 2-d poincare embedding.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : :class:`~hyperbolic.dag_emb_model.DAGEmbeddingModel`\n",
    "        The model to visualize, model size must be 2.\n",
    "    tree : list\n",
    "        Set of tuples containing the direct edges present in the original dataset.\n",
    "    figure_title : str\n",
    "        Title of the plotted figure.\n",
    "    num_nodes : int or None\n",
    "        Number of nodes for which edges are to be plotted.\n",
    "        If `None`, all edges are plotted.\n",
    "        Helpful to limit this in case the data is too large to avoid a messy plot.\n",
    "    show_node_labels : iterable\n",
    "        Iterable of nodes for which to show labels by default.\n",
    "    Returns\n",
    "    -------\n",
    "    :class:`plotly.graph_objs.Figure`\n",
    "        Plotly figure that contains plot.\n",
    "    \"\"\"\n",
    "    vectors = model.kv.syn0\n",
    "    if vectors.shape[1] != 2:\n",
    "        raise ValueError('Can only plot 2-D vectors')\n",
    "\n",
    "    node_labels = model.kv.index2word\n",
    "    nodes_x = list(vectors[:, 0])\n",
    "    nodes_y = list(vectors[:, 1])\n",
    "    nodes = dict(\n",
    "        x=nodes_x, y=nodes_y,\n",
    "        mode='markers',\n",
    "        marker=dict(color='rgb(30, 100, 200)'),\n",
    "        text=node_labels,\n",
    "        textposition='bottom'\n",
    "    )\n",
    "\n",
    "    nodes_x, nodes_y, node_labels = [], [], []\n",
    "    for node in show_node_labels:\n",
    "        if node in model.kv:\n",
    "            vector = model.kv[node]\n",
    "            nodes_x.append(vector[0])\n",
    "            nodes_y.append(vector[1])\n",
    "            node_labels.append(node)\n",
    "\n",
    "    nodes_with_labels = dict(\n",
    "        x=nodes_x, y=nodes_y,\n",
    "        mode='markers+text',\n",
    "        marker=dict(color='rgb(200, 100, 200)'),\n",
    "        text=node_labels,\n",
    "        textfont=dict(\n",
    "            family='sans serif',\n",
    "            size=18,\n",
    "            color='#ff7f0e' # orange\n",
    "        ),\n",
    "        textposition='bottom'\n",
    "    )\n",
    "\n",
    "    node_out_degrees = Counter(hypernym_pair[1] for hypernym_pair in tree)\n",
    "    if num_nodes is None:\n",
    "        chosen_nodes = list(node_out_degrees.keys())\n",
    "    else:\n",
    "        chosen_nodes = list(sorted(node_out_degrees.keys(), key=lambda k: -node_out_degrees[k]))[:num_nodes]\n",
    "\n",
    "    edges_x = []\n",
    "    edges_y = []\n",
    "    for u, v in tree:\n",
    "        if not(u in chosen_nodes or v in chosen_nodes):\n",
    "            continue\n",
    "        vector_u = model.kv[u]\n",
    "        vector_v = model.kv[v]\n",
    "        edges_x += [vector_u[0], vector_v[0], None]\n",
    "        edges_y += [vector_u[1], vector_v[1], None]\n",
    "    edges = dict(\n",
    "        x=edges_x, y=edges_y, mode=\"line\", hoverinfo=False,\n",
    "        line=dict(color='rgb(50,50,50)', width=1))\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=figure_title, showlegend=False, hovermode='closest', width=1500, height=1500,\n",
    "        xaxis={'range': [-1, 1.3], 'autorange': False},\n",
    "        yaxis={'range': [-1, 1.3], 'autorange': False},\n",
    "        updatemenus= [{'type': 'buttons',\n",
    "                         'buttons': [\n",
    "                             {'label': 'Play',\n",
    "                              'method': 'animate',\n",
    "                              'args': [None]\n",
    "                              },\n",
    "                              {\n",
    "                                 'args': [[None], {'frame': {'duration': 0, 'redraw': False},\n",
    "                                                   'mode': 'immediate',\n",
    "                                                   'transition': {'duration': 0}}],\n",
    "                                 'label': 'Pause',\n",
    "                                 'method': 'animate'\n",
    "                              }\n",
    "                         ]}]\n",
    "    )\n",
    "\n",
    "\n",
    "    epoch_sticker = dict(\n",
    "        x=[0.5], y = [1.2], mode='text', text=['Epoch : ' + str(epoch)],\n",
    "        textfont=dict(\n",
    "            family='sans serif',\n",
    "            size=20,\n",
    "            color='rgb(200,0,0)'\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    result_str = str(eval_result) + '<br>'\n",
    "    result_str += 'loss = %.2f; pos loss = %.2f; neg loss = %.2f' % (avg_loss, avg_pos_loss, avg_neg_loss)\n",
    "\n",
    "    eval_result_sticker = dict(\n",
    "        x=[0.5], y = [1.1],\n",
    "        mode='text',\n",
    "        text=[result_str],\n",
    "        textfont=dict(\n",
    "            family='sans serif',\n",
    "            size=20,\n",
    "            color='rgb(0,0,200)'\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add a new frame into the animation\n",
    "    frame = {'data': [], 'name': str(epoch)}\n",
    "    frame['data'].append(edges)\n",
    "    frame['data'].append(nodes_with_labels)\n",
    "    frame['data'].append(eval_result_sticker)\n",
    "    frame['data'].append(epoch_sticker)\n",
    "    animation['frames'].append(frame)\n",
    "\n",
    "    if epoch == 0:\n",
    "        animation['data'].append(edges)\n",
    "        animation['data'].append(nodes_with_labels)\n",
    "        animation['data'].append(eval_result_sticker)\n",
    "        animation['data'].append(epoch_sticker)\n",
    "\n",
    "    return go.Figure(data=[edges, nodes, nodes_with_labels, eval_result_sticker, epoch_sticker], layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes a list of relations and returns a list of relations\n",
    "# the relations are the same, but the order is different\n",
    "# the order is different because the relations are sorted by the number of ancestors\n",
    "# the relations with the most ancestors are first\n",
    "# the relations with the least ancestors are last\n",
    "# the relations with the most ancestors are the most general\n",
    "# the relations with the least ancestors are the most specific\n",
    "def recover_tree_from_transitive_closure(relations):\n",
    "    all_nodes_set = set()\n",
    "    for rel in relations:\n",
    "        all_nodes_set.add(rel[0])\n",
    "        all_nodes_set.add(rel[1])\n",
    "\n",
    "    ancestors = {}\n",
    "    for node in all_nodes_set:\n",
    "        ancestors[node] = []\n",
    "    for rel in relations:\n",
    "        if rel[0] != rel[1]:\n",
    "            ancestors[rel[1]].append(rel[0])\n",
    "\n",
    "    new_relations = []\n",
    "    for node in all_nodes_set:\n",
    "        num_ancestors = len(ancestors[node])\n",
    "        for ancestor in ancestors[node]:\n",
    "            if len(ancestors[ancestor]) == num_ancestors - 1:\n",
    "                new_relations.append((ancestor, node))\n",
    "\n",
    "    return new_relations\n",
    "\n",
    "data_file_path = 'data/mammals.csv'\n",
    "\n",
    "def read_tree_data():\n",
    "    # Load the tree data:\n",
    "    transitive_relations = []\n",
    "    tree_relations = recover_tree_from_transitive_closure(transitive_relations)\n",
    "\n",
    "    # Plot the embeddings\n",
    "    show_node_labels=[]\n",
    "    if opt_task == 'mammals':\n",
    "        show_node_labels=['dog.n.01', 'canine.n.02', 'carnivore.n.01', 'placental.n.01', # 'mammal.n.01',\n",
    "                          'rodent.n.01', 'clumber.n.01', 'ungulate.n.01', 'primate.n.02',\n",
    "                          'even-toed_ungulate.n.01', 'odd-toed_ungulate.n.01']\n",
    "\n",
    "    # All direct children of root\n",
    "    transitive_relations_without_root = []\n",
    "    tree_relations_without_root = []\n",
    "    root_label = 'mammal.n.01'\n",
    "    for rel in tree_relations:\n",
    "        if rel[0] != root_label:\n",
    "            tree_relations_without_root.append(rel)\n",
    "\n",
    "    for rel in transitive_relations:\n",
    "        if rel[0] != root_label:\n",
    "            transitive_relations_without_root.append(rel)\n",
    "\n",
    "    return transitive_relations_without_root, tree_relations_without_root, show_node_labels\n",
    "\n",
    "\n",
    "transitive_relations, tree_relations, show_node_labels = read_tree_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_lr(data, epoch, progress = False):\n",
    "    data.burnin = False \n",
    "    lr = opt_lr\n",
    "    if epoch < opt_burnin:\n",
    "        data.burnin = True\n",
    "        lr = opt_lr * train._lr_multiplier\n",
    "    loader_iter = tqdm(data) if progress else data\n",
    "    return loader_iter, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device, model, data, optimizer, progress=False):\n",
    "    epoch_loss = torch.Tensor(len(data))\n",
    "    LOSS = np.zeros(opt_epochs)\n",
    "    for epoch in range(opt_epochs):\n",
    "        epoch_loss.fill_(0)\n",
    "        t_start = timeit.default_timer()\n",
    "        # handling burnin, get loader_iter and learning rate\n",
    "        loader_iter, lr = data_loader_lr(data, epoch, progress=progress)\n",
    "        for i_batch, (inputs, targets) in enumerate(loader_iter):\n",
    "            elapsed = timeit.default_timer() - t_start\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(inputs) * opt_dscale\n",
    "            loss = model.loss(preds, targets, size_average=True)\n",
    "            loss.backward()\n",
    "            optimizer.step(lr=lr)\n",
    "            epoch_loss[i_batch] = loss.cpu().item()\n",
    "            with torch.no_grad():\n",
    "                loss = model.loss(preds.to(d64), targets, size_average=True)\n",
    "                epoch_loss[i_batch] = loss.cpu().item()\n",
    "\n",
    "            animation = create_animation(\"Visualization\")\n",
    "            \n",
    "            show_node_labels = True\n",
    "            figure_name = \"Visualization\"\n",
    "            figure = poincare_2d_visualization(\n",
    "                model,\n",
    "                animation=animation,\n",
    "                epoch=epoch,\n",
    "                eval_result='',\n",
    "                avg_loss=0,\n",
    "                avg_pos_loss=0,\n",
    "                avg_neg_loss=0,\n",
    "                tree=list(tree_relations),\n",
    "                show_node_labels=show_node_labels,\n",
    "                figure_title=figure_name,\n",
    "                num_nodes=None)    \n",
    "            figure.show()\n",
    "\n",
    "        LOSS[epoch] = torch.mean(epoch_loss).to(d64).item()\n",
    "        # since only one thread is used:\n",
    "        log.info('json_stats: {' f'\"epoch\": {epoch}, '\n",
    "                 f'\"elapsed\": {elapsed}, ' f'\"loss\": {LOSS[epoch]}, ' '}')\n",
    "                 \n",
    "    return LOSS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total dimension 2\n",
      ">>>>>> # Tensor# | dtype is: torch.float64 | device is: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15916/3908651642.py:4: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  data = BatchedDataset(idx, objects, weights, opt_negs, opt_batchsize,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'DAGEmbeddingKeyedVectors' has no attribute 'syn0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# begin training\u001b[39;00m\n\u001b[1;32m     15\u001b[0m start_time \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer()\n\u001b[0;32m---> 16\u001b[0m loss \u001b[39m=\u001b[39m train(device, model, data, optimizer, progress\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     17\u001b[0m train_time \u001b[39m=\u001b[39m timeit\u001b[39m.\u001b[39mdefault_timer() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal training time is:\u001b[39m\u001b[39m\"\u001b[39m, train_time)\n",
      "Cell \u001b[0;32mIn[54], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(device, model, data, optimizer, progress)\u001b[0m\n\u001b[1;32m     25\u001b[0m     show_node_labels \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     figure_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mVisualization\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 27\u001b[0m     figure \u001b[39m=\u001b[39m poincare_2d_visualization(\n\u001b[1;32m     28\u001b[0m         model,\n\u001b[1;32m     29\u001b[0m         animation\u001b[39m=\u001b[39;49manimation,\n\u001b[1;32m     30\u001b[0m         epoch\u001b[39m=\u001b[39;49mepoch,\n\u001b[1;32m     31\u001b[0m         eval_result\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m         avg_loss\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     33\u001b[0m         avg_pos_loss\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     34\u001b[0m         avg_neg_loss\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m         tree\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(tree_relations),\n\u001b[1;32m     36\u001b[0m         show_node_labels\u001b[39m=\u001b[39;49mshow_node_labels,\n\u001b[1;32m     37\u001b[0m         figure_title\u001b[39m=\u001b[39;49mfigure_name,\n\u001b[1;32m     38\u001b[0m         num_nodes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)    \n\u001b[1;32m     39\u001b[0m     figure\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     41\u001b[0m LOSS[epoch] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(epoch_loss)\u001b[39m.\u001b[39mto(d64)\u001b[39m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[51], line 78\u001b[0m, in \u001b[0;36mpoincare_2d_visualization\u001b[0;34m(model, animation, epoch, eval_result, avg_loss, avg_pos_loss, avg_neg_loss, tree, figure_title, num_nodes, show_node_labels)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpoincare_2d_visualization\u001b[39m(\n\u001b[1;32m     47\u001b[0m         model,\n\u001b[1;32m     48\u001b[0m         animation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m         num_nodes\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[1;32m     57\u001b[0m         show_node_labels\u001b[39m=\u001b[39m()):\n\u001b[1;32m     58\u001b[0m     \u001b[39m\"\"\"Create a 2-d plot of the nodes and edges of a 2-d poincare embedding.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39m        Plotly figure that contains plot.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     vectors \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mkv\u001b[39m.\u001b[39;49msyn0\n\u001b[1;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m vectors\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     80\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCan only plot 2-D vectors\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'DAGEmbeddingKeyedVectors' has no attribute 'syn0'"
     ]
    }
   ],
   "source": [
    "# setup model\n",
    "seed_everything(1)\n",
    "model, data, model_name = init_model(opt_manifold, opt_curvature, opt_dim, idx, objects, weights, sparse=opt_sparse)\n",
    "data.neg_multiplier = opt_neg_multiplier\n",
    "train._lr_multiplier = opt_burnin_multiplier\n",
    "model = model.to(device)\n",
    "print('the total dimension', model.lt.weight.data.size(-1))\n",
    "print(\">>>>>> # Tensor# | dtype is:\", model.lt.weight.dtype, \"| device is:\", model.lt.weight.device)\n",
    "# setup optimizer, both works, though a small lr should be used for RiemannianAdam (which is not tuned yet)\n",
    "#optimizer = RiemannianAdam(model.parameters(), lr=opt_lr)\n",
    "optimizer = RiemannianSGD(model.parameters(), lr=opt_lr)\n",
    "# get adjacency matrix\n",
    "adj = adj_matrix(data)\n",
    "# begin training\n",
    "start_time = timeit.default_timer()\n",
    "loss = train(device, model, data, optimizer, progress=False)\n",
    "train_time = timeit.default_timer() - start_time\n",
    "print(\"Total training time is:\", train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RES():\n",
    "    \"\"\"for logging results\"\"\"\n",
    "    def __init__(self, loss, eval_res, weight):\n",
    "        self.loss = torch.tensor(loss, dtype=torch.float64, \n",
    "                                 device=cpu)\n",
    "        self.eval_res = torch.tensor(eval_res, dtype=torch.float64, \n",
    "                                     device=cpu)\n",
    "        self.weight = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_stats final test: \n",
      "{\"sqnorm_min\": 0.000141, \"sqnorm_avg\": 0.005579, \"sqnorm_max\": 0.040499, \n",
      "\"mean_rank\": 503.558257, \"map\": 0.019105, }\n"
     ]
    }
   ],
   "source": [
    "model_weight = model.lt.weight.clone()\n",
    "# eval meanrank, maprank in the original model\n",
    "meanrank, maprank = eval_reconstruction(adj, model_weight, workers=opt_ndproc)\n",
    "if opt_manifold != \"PoincareBall\":\n",
    "    # change to PoincareBall to derive the sqnorms metric\n",
    "    model_weight = model_weight.to_other_manifold(\"PoincareBall\")\n",
    "sqnorms = torch.sqrt(torch.sum(torch.pow(model_weight, 2), dim=-1))\n",
    "sqnorm_min = sqnorms.min().item()\n",
    "sqnorm_avg = sqnorms.mean().item()\n",
    "sqnorm_max = sqnorms.max().item()\n",
    "eval_res = [meanrank, maprank, sqnorm_min, sqnorm_avg, sqnorm_max, train_time]\n",
    "RESULTS = RES(loss, eval_res, model_weight)\n",
    "# torch.save(RESULTS, \"./results_weights/\"+FILE_NAME+\"_seed1\"+ \".pt\")\n",
    "log.info(\n",
    "    'json_stats final test: \\n{'\n",
    "    f'\"sqnorm_min\": {round(sqnorm_min,6)}, '\n",
    "    f'\"sqnorm_avg\": {round(sqnorm_avg,6)}, '\n",
    "    f'\"sqnorm_max\": {round(sqnorm_max,6)}, \\n'\n",
    "    f'\"mean_rank\": {round(meanrank,6)}, '\n",
    "    f'\"map\": {round(maprank,6)}, '\n",
    "    '}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ht",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
